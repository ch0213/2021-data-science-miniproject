{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/uni2237/Data_Analysis/blob/master/%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project - 수입 우범화물 선별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fwk8iYLMLZdA"
   },
   "source": [
    "# 1. 분석 환경 설정 및 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qRAVMfnICXiC"
   },
   "outputs": [],
   "source": [
    "# 모델 구현 시간 체크\n",
    "import time\n",
    "\n",
    "# 시작시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 패키지 로딩\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글폰트 적용\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "# 수입 데이터 로딩 (가상데이터)\n",
    "train_xlsx = pd.read_excel(\"train.xlsx\")\n",
    "train_xlsx.to_csv(\"train.csv\")\n",
    "df_org = pd.read_csv('train.csv',index_col=[0])\n",
    "\n",
    "test_xlsx = pd.read_excel(\"test.xlsx\")\n",
    "test_xlsx.to_csv(\"test.csv\")\n",
    "df_org_ts=pd.read_csv('test.csv',index_col=[0]) #unnamed 삭제 후 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbu-MZQaLg-K"
   },
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z8r3xOKLsD9"
   },
   "source": [
    "### 2.1 원산-적출 컬럼 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PRCNdtx-GEsj"
   },
   "outputs": [],
   "source": [
    "# 원산지-적출 컬럼 생성\n",
    "df_org[\"원산지적출\"] = df_org[\"원산지국가코드\"] +df_org[\"적출국가코드\"]\n",
    "df_org_ts[\"원산지적출\"] = df_org_ts[\"원산지국가코드\"] +df_org_ts[\"적출국가코드\"]\n",
    "\n",
    "\n",
    "# 원산지 / 적출 / 원산지적출 리스트 생성\n",
    "def make_list(df_org):\n",
    "    result_1=df_org['원산지국가코드'][df_org['우범여부']==1].value_counts()\n",
    "    result_0=df_org['원산지국가코드'][df_org['우범여부']==0].value_counts()\n",
    "\n",
    "    result_11=df_org['적출국가코드'][df_org['우범여부']==1].value_counts()\n",
    "    result_01=df_org['적출국가코드'][df_org['우범여부']==0].value_counts()\n",
    "\n",
    "\n",
    "    result_all = df_org['원산지국가코드'].value_counts()\n",
    "    result_all1 = df_org['적출국가코드'].value_counts()\n",
    "\n",
    "  \n",
    "    result_12=df_org['원산지적출'][df_org['우범여부']==1].value_counts()\n",
    "    result_02=df_org['원산지적출'][df_org['우범여부']==0].value_counts()\n",
    "    result_all2 = df_org['원산지적출'].value_counts()\n",
    "\n",
    "    a_b=[]\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in result_1.keys():\n",
    "      #print(i,\"의 우범비율 %.2f%%\" % (result_1[i] / result_all[i]* 100.0), \"   (\",result_all[i],\"개 중 \",result_1[i],\")\")\n",
    "      if int(result_1[i] / result_all[i]* 100.0)>=50:\n",
    "        a.append(i)\n",
    "\n",
    "    for i in result_11.keys():\n",
    "        #print(i,\"의 우범비율 %.2f%%\" % (result_11[i] / result_all1[i]* 100.0), \"   (\",result_all1[i],\"개 중 \",result_11[i],\")\")\n",
    "        if int(result_11[i] / result_all1[i]* 100.0)>=50:\n",
    "          b.append(i)\n",
    "    for i in sorted(result_12.keys()):\n",
    "        #print(i,\"의 우범비율 %.2f%%\" % (result_12[i] / result_all2[i]* 100.0), \"   (\",result_all2[i],\"개 중 \",result_12[i],\")\")\n",
    "        if int(result_12[i] / result_all2[i]* 100.0)>=70:\n",
    "          a_b.append(i)\n",
    "    print(\"a\",a)\n",
    "    print(\"b\",b)\n",
    "    print(\"a_b\",a_b)\n",
    "    print(len(a_b))\n",
    "\n",
    "    #신고인 부호 리스트 생성\n",
    "    big_list=[]  # 1로 바꿔줄 신고인부호 저장할 리스트 (우범 횟수가 100이상 또는 우범 비열이 50% 이상인 신고인들을 1로 둠)\n",
    "    result_1=df_org['신고인부호'][df_org['우범여부']==1].value_counts()\n",
    "    result_all=df_org['신고인부호'].value_counts()\n",
    "    count=0\n",
    "    for i in result_1:\n",
    "        if i>=100:\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for i in result_1.keys()[0:count]:\n",
    "        big_list.append(i)\n",
    "\n",
    "    for i in sorted(result_1.keys()):\n",
    "        if (result_1[i] / result_all[i]) * 100 >=50:\n",
    "            big_list.append(i)\n",
    "\n",
    "    return a,b,a_b,big_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pU8tlSyLvcu"
   },
   "source": [
    "### 2.2 원산지-적출 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kk8V6FI0Afid"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ['BA', 'UA', 'HR', 'AM', 'BO', 'GT']\n",
      "b ['IE', 'AR', 'LI']\n",
      "a_b ['AMIT', 'ARAR', 'ATTW', 'BADE', 'CNAT', 'CNTR', 'ESSG', 'GBJP', 'GBNL', 'GTNL', 'HRVN', 'INMY', 'JPPL', 'KRMX', 'PTDE', 'THGB', 'THVN', 'TRNL']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "a,b,a_b,big_list=make_list(df_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3DMU0hDL_Ue"
   },
   "source": [
    "### 2.3 이 후 전처리 기능을 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JiGhqf5ICXiF"
   },
   "outputs": [],
   "source": [
    "def data_process(df_org):\n",
    "    \n",
    "    \n",
    "    df_org = pd.get_dummies(df_org, columns = ['통관지세관부호', '징수형태코드', '운송수단유형코드'])\n",
    "    \n",
    "  \n",
    "  # 원산지 / 적출 / 원산지 적출\n",
    "\n",
    "    def function_a(row):\n",
    "        if row['원산지국가코드'] in a:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def function_b(row):\n",
    "        if row['적출국가코드'] in b:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def function_a_b(row):\n",
    "        if row['원산지적출'] in a_b:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    df_org['원산지국가코드'] =df_org.apply(function_a,axis=1)\n",
    "    df_org['적출국가코드'] =df_org.apply(function_b,axis=1)\n",
    "    df_org['원산지적출'] =df_org.apply(function_a_b,axis=1)\n",
    "\n",
    "\n",
    "  # 수치형 변수를 자연로그로 변환(13, 14)\n",
    "    for var in ['신고중량(KG)', '과세가격원화금액']:\n",
    "        df_org[var] = df_org[var].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    \n",
    "  # 신고 날짜 전처리를 위해 함수 정의\n",
    "  #  def extract_month(row): \n",
    "  #      if int(row.split('-')[1]) <= 6:\n",
    "  #          return 0\n",
    "  #      else:\n",
    "  #          return 1\n",
    "  #  def extract_day(row) : return int(row.split('-')[2])//10\n",
    "    def extract_day(row) : return int(row.split('-')[2])//10\n",
    "\n",
    "\n",
    "#   # 2.신고 일자\n",
    "#     df_org['월'] = df_org['신고일자'].apply(extract_month)\n",
    "    df_org['일'] = df_org['신고일자'].apply(extract_day)\n",
    "    del df_org['신고일자']\n",
    "    df_org = pd.get_dummies(df_org, columns = ['일'])\n",
    "\n",
    "    \n",
    "  # 4.신고인부호\n",
    "    def function_man(row):\n",
    "        if row['신고인부호'] in big_list:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    df_org['신고인부호'] =df_org.apply(function_man,axis=1)\n",
    "\n",
    "\n",
    "    # 6.해외거래처부호\n",
    "    df = df_org['해외거래처부호'].fillna('missing') # nan을 missing으로 치환\n",
    "    df_org['해외거래처부호'] = df \n",
    "    df_org=pd.get_dummies(df_org, columns = ['해외거래처부호'])\n",
    "    missing = df_org.filter(regex='^해외거래처부호_missing') \n",
    "    df_org.drop(df_org.filter(regex='^해외거래처부호_'),axis=1,inplace=True) #missing 컬럼 제외하고 다지우기\n",
    "    df_org['해외거래처부호_missing'] =  missing\n",
    "\n",
    "\n",
    "    # 7.특송업체번호\n",
    "    df = df_org['특송업체부호'].fillna('missing') # nan을 missing으로 치환\n",
    "    df_org['특송업체부호'] = df \n",
    "    df_org=pd.get_dummies(df_org, columns = ['특송업체부호'])\n",
    "    missing = df_org.filter(regex='^특송업체부호_missing') \n",
    "    df_org.drop(df_org.filter(regex='^특송업체부호_'),axis=1,inplace=True) #missing 컬럼 제외하고 다지우기\n",
    "    df_org['특송업체부호_missing'] =  missing\n",
    "\n",
    "\n",
    "    # 8.수입통관계획코드\n",
    "    df_org=pd.get_dummies(df_org, columns = ['수입통관계획코드'])\n",
    "    df_org['수입통관계획코드AG'] = 0\n",
    "    df_org[\"수입통관계획코드CD\"] = df_org[\"수입통관계획코드_C\"] +df_org[\"수입통관계획코드_D\"]\n",
    "    df_org[\"수입통관계획코드EF\"] = df_org[\"수입통관계획코드_E\"] +df_org[\"수입통관계획코드_F\"]\n",
    "    df_org[\"수입통관계획코드HZ\"] = df_org[\"수입통관계획코드_H\"] +df_org[\"수입통관계획코드_Z\"]\n",
    "    df_org.drop(df_org.filter(regex='^수입통관계획코드_'),axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    # 9.수입신고구분코드\n",
    "    df_org=pd.get_dummies(df_org, columns = ['수입신고구분코드'])\n",
    "    df_org['수입신고구분코드CFG'] = 0\n",
    "\n",
    "\n",
    "    # 10.수입거래구분코드\n",
    "    df_org['수입거래구분코드_십'] = df_org['수입거래구분코드'] // 10\n",
    "    df_org['수입거래구분코드_일'] = df_org['수입거래구분코드'] % 10\n",
    "    del df_org['수입거래구분코드']\n",
    "    df_org = pd.get_dummies(df_org, columns = ['수입거래구분코드_십', '수입거래구분코드_일'])\n",
    "\n",
    "\n",
    "    # 11.수입종류코드(\n",
    "    df_org.loc[df_org['수입종류코드'] == 21, '수입종류코드'] = 0\n",
    "    df_org.loc[df_org['수입종류코드'] == 11, '수입종류코드'] = 0\n",
    "    df_org.loc[df_org['수입종류코드'] != 0, '수입종류코드'] = 1\n",
    "    df_org = pd.get_dummies(df_org, columns = ['수입종류코드'])\n",
    "\n",
    "\n",
    "    # 16.반입보세구역부호\n",
    "    def extract_1(row): return str(row)[0]\n",
    "    def extract_2(row): return str(row)[1]\n",
    "    def extract_3(row): return str(row)[0]\n",
    "    def extract_4(row): return str(row)[1]\n",
    "    df_org['반입보세구역부호_1'] = df_org['반입보세구역부호'].apply(extract_1)\n",
    "    df_org['반입보세구역부호_2'] = df_org['반입보세구역부호'].apply(extract_2)\n",
    "    df_org['반입보세구역부호_3'] = df_org['반입보세구역부호'].apply(extract_3)\n",
    "    df_org['반입보세구역부호_4'] = df_org['반입보세구역부호'].apply(extract_4)\n",
    "    df_org = pd.get_dummies(df_org, columns = ['반입보세구역부호_1', '반입보세구역부호_2', '반입보세구역부호_3', '반입보세구역부호_4'])\n",
    "    del df_org['반입보세구역부호']\n",
    "\n",
    "    \n",
    "    # 17-1. HS10단위 부호(주요 코드)\n",
    "    df_hs=df_org[['HS10단위부호']]/100000000\n",
    "    df_hs=df_hs.astype(int)\n",
    "    df_org['HS10단위']=df_hs\n",
    "     \n",
    "    #아래 주석에서 만들어진 hs_list가 주석 하단의 main_hs 입니다.\n",
    "    #result_h1=df_org['HS10단위'][df_org['우범여부']==1].value_counts()\n",
    "    #result_h0=df_org['HS10단위'][df_org['우범여부']==0].value_counts()\n",
    "    #result_allh = df_org['HS10단위'].value_counts()\n",
    "    #hs_list=[]\n",
    "    #for i in sorted(result_h1.keys()):\n",
    "    #    print(i,\"의 우범비율 %.2f%%\" % (result_h1[i] / result_allh[i]* 100.0), \"   (\",result_allh[i],\"개 중 \",result_h1[i],\")\")\n",
    "    #    if int(result_h1[i] / result_allh[i]* 100.0)>=50:\n",
    "    #        hs_list.append(i)\n",
    "    \n",
    "    \n",
    "    main_hs=[36, 78, 14, 93, 79]\n",
    "    def function_hs(row):\n",
    "        if row['HS10단위'] in main_hs:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    df_org['HS10단위'] =df_org.apply(function_hs,axis=1)\n",
    "    \n",
    "    \n",
    "    # 17-2.HS10단위부호\n",
    "    df_org['HS10단위부호_1'] = df_org['HS10단위부호'].apply(extract_1)\n",
    "    df_org['HS10단위부호_2'] = df_org['HS10단위부호'].apply(extract_2)\n",
    "    df_org = pd.get_dummies(df_org, columns = ['HS10단위부호_1', 'HS10단위부호_2'])\n",
    "    del df_org['HS10단위부호']\n",
    "    \n",
    "\n",
    "\n",
    "    # 20.관세율 구분 코드\n",
    "    df_org=pd.get_dummies(df_org, columns = ['관세율구분코드']) # 관세율구분코드를 원핫인코딩으로 변환\n",
    "    df_org[\"관세율구분_A\"]=df_org.iloc[:,-35]   # 각 범위로 그룹화,\n",
    "    df_org[\"관세율구분_CE\"]=df_org.iloc[:,-35:-30].sum(axis=1)\n",
    "    df_org[\"관세율구분_FC\"]=df_org.iloc[:,-31:-22].sum(axis=1)\n",
    "    df_org[\"관세율구분_FE\"]=df_org.iloc[:,-23:-9].sum(axis=1)\n",
    "    df_org[\"관세율구분_LP\"]=df_org.iloc[:,-10:-7].sum(axis=1)\n",
    "    df_org[\"관세율구분_RW\"]=df_org.iloc[:,-8:-5].sum(axis=1)\n",
    "    df_org.drop(df_org.filter(regex='^관세율구분코드_'),axis=1,inplace=True)\n",
    "\n",
    "    # 1 2 3 4 [5] 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [21] 22 23\n",
    "\n",
    "  \n",
    "\n",
    "    del df_org['수입자부호']\n",
    "    del df_org['관세율']\n",
    "    return df_org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEngHh5_MUVC"
   },
   "source": [
    "### 2.4 전처리 결과 저장 및 불필요한 변수 삭제 (trian.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C9UPv7HyCXiM"
   },
   "outputs": [],
   "source": [
    "# 불필요한 변수 삭제(1, 22, 23)\n",
    "del df_org['신고번호']\n",
    "del df_org['검사결과코드']\n",
    "\n",
    "df_org_train=data_process(df_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSwtZcYUMXhA"
   },
   "source": [
    "### 2.5 전처리 결과 저장 및 불필요한 변수 삭제 (test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KevuXyd_5tLa",
    "outputId": "e8c0f5bd-7923-4409-d5a3-90d83c395cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape : (76837, 158)\n",
      "test_data shape : (23163, 156)\n"
     ]
    }
   ],
   "source": [
    "del df_org_ts['신고번호']\n",
    "df_org_ts=data_process(df_org_ts)\n",
    "test_data = df_org_ts\n",
    "\n",
    "print(\"train_data shape :\" ,df_org_train.shape)\n",
    "print(\"test_data shape :\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cyOlb8UPtKA"
   },
   "source": [
    "### 2.6 Class-imbalanced 해결 : SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0tsc8PaPyTB",
    "outputId": "fa97df8c-f0fc-4f03-b6c3-bacd97b6e17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (76837, 157) (76837,)\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (118530, 156) (118530,)\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      " 1    59265\n",
      "0    59265\n",
      "Name: 우범여부, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "\n",
    "org_train_y = df_org_train.pop('우범여부')\n",
    "x,y = smote.fit_resample(df_org_train, org_train_y)\n",
    "core_y = x.pop('핵심적발')\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', df_org_train.shape, org_train_y.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', x.shape, y.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMs-RTTFP1x9"
   },
   "source": [
    "# 3. 모델 생성 및 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwXNG3B_P5wi"
   },
   "source": [
    "### 3.1 모델, K-Fold, Score(acc, recall, precision, f1) Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T3TPKHnCQEqU"
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "# K-Fold\n",
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Acc, F1 score\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1S4fXo3QGzp"
   },
   "source": [
    "### 3.2 K-Fold로 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBwmjX4aYAZW"
   },
   "source": [
    "### 3.3 score를 저장할 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3MwZ8wXICXiO"
   },
   "outputs": [],
   "source": [
    "score = {\n",
    "    'RF' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "    'XGBOOST' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "    'GBM' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "    'DNN' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "43ZA3WQhCXiP"
   },
   "outputs": [],
   "source": [
    "def CreateModel(length):\n",
    "    \n",
    "    input_shape = (length,)\n",
    "    mlp_model = models.Sequential()\n",
    "    mlp_model.add(layers.Dense(units = 10, activation = 'relu', input_shape=input_shape))\n",
    "    mlp_model.add(layers.Dense(units = 20, activation = 'relu'))\n",
    "    mlp_model.add(layers.Dense(units = 20, activation = 'relu'))\n",
    "    mlp_model.add(layers.Dense(units = 2, activation = 'softmax'))\n",
    "    mlp_model.compile(optimizer='Adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "    return mlp_model\n",
    "\n",
    "RF=RandomForestClassifier(max_depth=22, n_estimators=300, random_state=0,n_jobs=-1)\n",
    "XGBOOST= XGBClassifier(n_estimators=10, max_depth=4)\n",
    "GBM = GradientBoostingClassifier(max_depth=3, n_estimators=30, random_state=0)\n",
    "#DNN = CreateModel(df_org_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 학습 및 Score 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "FIely669YJVS",
    "outputId": "46134bd1-5792-4c1b-fded-54952578ca29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [06:18, 75.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1. RandomForestClassifier [평균 Accuracy] : 85.54036952670211 ,  [Accuracy 편차] : 0.0027103163228880357\n",
      "1-2. RandomForestClassifier [평균 Precision] : 88.05180874471581 , [Precision 편차]: 0.0039910347859484704\n",
      "1-3. RandomForestClassifier [평균 Recall] : 83.84050740154191 ,    [Recall 편차]   : 0.003353785132827144\n",
      "1-4. RandomForestClassifier [평균 F1_score] : 85.89399618319369 ,   [F1_score 편차] : 0.0029179670045714826 \n",
      "\n",
      "2-1. XGBOOSTClassifier [평균 Accuracy] : 80.25394414916053 ,  [Accuracy 편차] : 0.0037068637094094512\n",
      "2-2. XGBOOSTClassifier [평균 Precision] : 82.90629591280985 , [Precision 편차] : 0.005977888972653222\n",
      "2-3. XGBOOSTClassifier [평균 Recall] : 78.73079346294915 ,   [Recall 편차] : 0.0036349968336406866\n",
      "2-4. XGBOOSTClassifier [평균 F1_score] : 80.76348134365875 ,  [F1_score 편차] : 0.003810183003523968 \n",
      "\n",
      "3-1. GradientBoostingClassifier [평균 Accuracy] : 79.34953176411035 ,  [Accuracy 편차] : 0.0028358550737391736\n",
      "3-2. GradientBoostingClassifier [평균 Precision] : 81.32917544482127 , [Precision 편차] : 0.004671806107954319\n",
      "3-3. GradientBoostingClassifier [평균 Recall]   : 78.23132373974346 ,    [Recall 편차] : 0.003243060076857255\n",
      "3-4. GradientBoostingClassifier [평균 F1_score] : 79.74953665953446 ,   [F1_score 편차] : 0.0032839636299211153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습, 예측\n",
    "for train_index, test_index in tqdm(kf.split(x, y)):\n",
    "    x_train, x_test, y_train, y_test=\\\n",
    "    x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    RF.fit(x_train, y_train.ravel())\n",
    "    XGBOOST.fit(x_train, y_train.ravel(), eval_metric=[\"logloss\"], verbose=True)\n",
    "    GBM.fit(x_train, y_train.ravel())\n",
    "#     DNN.fit(x_train, encoder.fit_transform(y_train), batch_size=10, epochs=50)\n",
    "    \n",
    "    score['RF']['accuracy'].append(RF.score(x_test, y_test.values.ravel()))\n",
    "    score['RF']['precision'].append(precision_score(RF.predict(x_test), y_test))\n",
    "    score['RF']['recall'].append(recall_score(RF.predict(x_test), y_test))\n",
    "    score['RF']['f1_score'].append(f1_score(RF.predict(x_test), y_test))\n",
    "    \n",
    "    score['XGBOOST']['accuracy'].append(XGBOOST.score(x_test, y_test.values.ravel()))\n",
    "    score['XGBOOST']['precision'].append(precision_score(XGBOOST.predict(x_test), y_test))\n",
    "    score['XGBOOST']['recall'].append(recall_score(XGBOOST.predict(x_test), y_test))\n",
    "    score['XGBOOST']['f1_score'].append(f1_score(XGBOOST.predict(x_test), y_test))\n",
    "    \n",
    "    score['GBM']['accuracy'].append(GBM.score(x_test, y_test.values.ravel()))\n",
    "    score['GBM']['precision'].append(precision_score(GBM.predict(x_test), y_test))\n",
    "    score['GBM']['recall'].append(recall_score(GBM.predict(x_test), y_test))\n",
    "    score['GBM']['f1_score'].append(f1_score(GBM.predict(x_test), y_test))\n",
    "    \n",
    "#     score['DNN']['accuracy'].append(DNN.score(x_test, y_test.values.ravel()))\n",
    "#     score['DNN']['precision'].append(precision_score(DNN.predict(x), y))\n",
    "#     score['DNN']['recall'].append(recall_score(DNN.predict(x), y))\n",
    "#     score['DNN']['f1_score'].append(f1_score(DNN.predict(x), y))\n",
    "\n",
    "    \n",
    "# Score 출력\n",
    "print(f'1-1. RandomForestClassifier [평균 Accuracy] :', np.mean(score['RF']['accuracy'])*100 , ',  [Accuracy 편차] :', np.std(score['RF']['accuracy']))\n",
    "print(f'1-2. RandomForestClassifier [평균 Precision] :', np.mean(score['RF']['precision'])*100 , ', [Precision 편차]:', np.std(score['RF']['precision']))\n",
    "print(f'1-3. RandomForestClassifier [평균 Recall] :', np.mean(score['RF']['recall'])*100 , ',    [Recall 편차]   :', np.std(score['RF']['recall']))\n",
    "print(f'1-4. RandomForestClassifier [평균 F1_score] :', np.mean(score['RF']['f1_score'])*100 , ',   [F1_score 편차] :', np.std(score['RF']['f1_score']),'\\n')\n",
    "\n",
    "print(f'2-1. XGBOOSTClassifier [평균 Accuracy] :', np.mean(score['XGBOOST']['accuracy'])*100 , ',  [Accuracy 편차] :', np.std(score['XGBOOST']['accuracy']))\n",
    "print(f'2-2. XGBOOSTClassifier [평균 Precision] :', np.mean(score['XGBOOST']['precision'])*100 , ', [Precision 편차] :', np.std(score['XGBOOST']['precision']))\n",
    "print(f'2-3. XGBOOSTClassifier [평균 Recall] :', np.mean(score['XGBOOST']['recall'])*100 , ',   [Recall 편차] :', np.std(score['XGBOOST']['recall']))\n",
    "print(f'2-4. XGBOOSTClassifier [평균 F1_score] :', np.mean(score['XGBOOST']['f1_score'])*100 , ',  [F1_score 편차] :', np.std(score['XGBOOST']['f1_score']),'\\n')\n",
    "\n",
    "print(f'3-1. GradientBoostingClassifier [평균 Accuracy] :', np.mean(score['GBM']['accuracy'])*100 , ',  [Accuracy 편차] :', np.std(score['GBM']['accuracy']))\n",
    "print(f'3-2. GradientBoostingClassifier [평균 Precision] :', np.mean(score['GBM']['precision'])*100 , ', [Precision 편차] :', np.std(score['GBM']['precision']))\n",
    "print(f'3-3. GradientBoostingClassifier [평균 Recall]   :', np.mean(score['GBM']['recall'])*100 , ',    [Recall 편차] :', np.std(score['GBM']['recall']))\n",
    "print(f'3-4. GradientBoostingClassifier [평균 F1_score] :', np.mean(score['GBM']['f1_score'])*100 , ',   [F1_score 편차] :', np.std(score['GBM']['f1_score']))\n",
    "                                                                                                                          \n",
    "# print(f'1-2. DeepNeuralNetworkClassifier [평균 Accuracy] :', np.mean(score['DNN']['accuracy'])*100 , ',  [Accuracy 편차] :', np.std(score['DNN']['accuracy']))\n",
    "# print(f'1-2. DeepNeuralNetworkClassifier [평균 Precision] :', np.mean(score['DNN']['precision'])*100 , ', [Precision 편차] :', np.std(score['DNN']['precision']))\n",
    "# print(f'1-2. DeepNeuralNetworkClassifier [평균 Recall]   :', np.mean(score['DNN']['recall'])*100 , ',    [Recall 편차] :', np.std(score['DNN']['recall']))\n",
    "# print(f'1-2. DeepNeuralNetworkClassifier [평균 F1_score] :', np.mean(score['DNN']['f1_score'])*100 , ',   [F1_score 편차] :', np.std(score['DNN']['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 핵심적발을 예측하는 모델의 Score를 저장할 dict 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2 = {\n",
    "    'RF' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "    'XGBOOST' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "    'GBM' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "    'DNN' :{\n",
    "        'accuracy' : [], 'precision' : [], 'recall' : [], 'f1_score' : [],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 모델 생성 및 학습, Score 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [03:38, 43.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1. RandomForestClassifier [평균 Accuracy] : 86.6422002868472 ,  [Accuracy 편차] : 0.0030766257974661907\n",
      "1-2. RandomForestClassifier [평균 Precision] : 86.6422002868472 , [Precision 편차]: 0.0030766257974661907\n",
      "1-3. RandomForestClassifier [평균 Recall] : 86.6422002868472 ,    [Recall 편차]   : 0.0030766257974661907\n",
      "1-4. RandomForestClassifier [평균 F1_score] : 86.6422002868472 ,   [F1_score 편차] : 0.00307662579746615 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 생성\n",
    "RF_2=RandomForestClassifier(max_depth=22, n_estimators=300, random_state=0,n_jobs=-1)\n",
    "\n",
    "x['우범여부'] = y\n",
    "y=core_y\n",
    "\n",
    "# 학습, 예측\n",
    "for train_index, test_index in tqdm(kf.split(x, y)):\n",
    "    x_train, x_test, y_train, y_test=\\\n",
    "    x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    RF_2.fit(x_train, y_train.ravel())\n",
    "    score_2['RF']['accuracy'].append(RF_2.score(x_test, y_test.values.ravel()))\n",
    "    score_2['RF']['precision'].append(precision_score(RF_2.predict(x_test), y_test, average='micro'))\n",
    "    score_2['RF']['recall'].append(recall_score(RF_2.predict(x_test), y_test, average='micro'))\n",
    "    score_2['RF']['f1_score'].append(f1_score(RF_2.predict(x_test), y_test, average='micro'))\n",
    "    \n",
    "print(f'1-1. RandomForestClassifier [평균 Accuracy] :', np.mean(score_2['RF']['accuracy'])*100 , ',  [Accuracy 편차] :', np.std(score_2['RF']['accuracy']))\n",
    "print(f'1-2. RandomForestClassifier [평균 Precision] :', np.mean(score_2['RF']['precision'])*100 , ', [Precision 편차]:', np.std(score_2['RF']['precision']))\n",
    "print(f'1-3. RandomForestClassifier [평균 Recall] :', np.mean(score_2['RF']['recall'])*100 , ',    [Recall 편차]   :', np.std(score_2['RF']['recall']))\n",
    "print(f'1-4. RandomForestClassifier [평균 F1_score] :', np.mean(score_2['RF']['f1_score'])*100 , ',   [F1_score 편차] :', np.std(score_2['RF']['f1_score']),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Test data Load, Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xlsx = pd.read_excel(\"test.xlsx\")\n",
    "test_xlsx.to_csv(\"test.csv\")\n",
    "df_org_ts=pd.read_csv('test.csv',index_col=[0]) #unnamed 삭제 후 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "# test data preprocessing\n",
    "df_org_ts[\"원산지적출\"] = df_org_ts[\"원산지국가코드\"] + df_org_ts[\"적출국가코드\"]\n",
    "df_org_ts[\"원산지적출\"] = df_org_ts[\"원산지국가코드\"] + df_org_ts[\"적출국가코드\"]\n",
    "num = df_org_ts.pop('신고번호')\n",
    "df_org_ts = data_process(df_org_ts)\n",
    "\n",
    "# predict\n",
    "result_1 = RF.predict(df_org_ts)\n",
    "df_org_ts['우범여부']= result_1\n",
    "result_2 = RF_2.predict(df_org_ts)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"신고번호\" : num,\n",
    "    \"우범여부\" : result_1,\n",
    "    \"핵심적발\" : result_2\n",
    "})\n",
    "submission.to_excel('RF_submission.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "전처리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
